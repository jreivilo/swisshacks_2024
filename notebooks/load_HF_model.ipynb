{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore how to use a model that is hosted on Hugging Face\n",
    "#### With the \"transformers\" library from Hugging Face there are two possible ways to do inference allowing for different levels of control:\n",
    "* **Option 1:** Using the \"pipeline\" function, allowing for fast model setup\n",
    "* **Option 2:** Sequential function calls for tokenisation, generation and subsequent decoding allowing you to specify more things in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed download the packages\n",
    "# !pip install transformers huggingface_hub torch gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-17T07:14:21.762398Z",
     "iopub.status.busy": "2024-06-17T07:14:21.762098Z",
     "iopub.status.idle": "2024-06-17T07:14:39.249624Z",
     "shell.execute_reply": "2024-06-17T07:14:39.248733Z",
     "shell.execute_reply.started": "2024-06-17T07:14:21.762358Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:14:39.251808Z",
     "iopub.status.busy": "2024-06-17T07:14:39.251270Z",
     "iopub.status.idle": "2024-06-17T07:14:40.327825Z",
     "shell.execute_reply": "2024-06-17T07:14:40.326750Z",
     "shell.execute_reply.started": "2024-06-17T07:14:39.251783Z"
    }
   },
   "outputs": [],
   "source": [
    "# check GPU availability\n",
    "# if you selected \"GPU T4 x2\", then you should see 2 instances of Tesla T4 with 15GB each.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Using the pipeline function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:14:40.329654Z",
     "iopub.status.busy": "2024-06-17T07:14:40.329336Z",
     "iopub.status.idle": "2024-06-17T07:17:12.347422Z",
     "shell.execute_reply": "2024-06-17T07:17:12.346658Z",
     "shell.execute_reply.started": "2024-06-17T07:14:40.329623Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Pipeline\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)\n",
    "model = pipeline(task = \"text-generation\",\n",
    "                model = \"Qwen/Qwen2-7B-Instruct\",\n",
    "                device_map = \"auto\",\n",
    "                torch_dtype = torch.float16) # specify precision for model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:17:12.349716Z",
     "iopub.status.busy": "2024-06-17T07:17:12.349404Z",
     "iopub.status.idle": "2024-06-17T07:17:13.486236Z",
     "shell.execute_reply": "2024-06-17T07:17:13.485113Z",
     "shell.execute_reply.started": "2024-06-17T07:17:12.349668Z"
    }
   },
   "outputs": [],
   "source": [
    "# check that model is loaded on the GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:17:13.488340Z",
     "iopub.status.busy": "2024-06-17T07:17:13.487942Z",
     "iopub.status.idle": "2024-06-17T07:17:30.593219Z",
     "shell.execute_reply": "2024-06-17T07:17:30.592334Z",
     "shell.execute_reply.started": "2024-06-17T07:17:13.488299Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt the model\n",
    "output = model(\"Explain how to win an Hackathon using AI\", max_new_tokens = 250)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:17:30.594806Z",
     "iopub.status.busy": "2024-06-17T07:17:30.594502Z",
     "iopub.status.idle": "2024-06-17T07:17:31.237938Z",
     "shell.execute_reply": "2024-06-17T07:17:31.237162Z",
     "shell.execute_reply.started": "2024-06-17T07:17:30.594781Z"
    }
   },
   "outputs": [],
   "source": [
    "# delete model from GPU\n",
    "del model #deleting the model \n",
    "\n",
    "# model will still be on cache until its place is taken by other objects\n",
    "# so also execute the below lines\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:17:31.239292Z",
     "iopub.status.busy": "2024-06-17T07:17:31.239032Z",
     "iopub.status.idle": "2024-06-17T07:17:32.344078Z",
     "shell.execute_reply": "2024-06-17T07:17:32.342984Z",
     "shell.execute_reply.started": "2024-06-17T07:17:31.239271Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Manually do model loading, tokenisation, generation and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:17:32.345945Z",
     "iopub.status.busy": "2024-06-17T07:17:32.345612Z",
     "iopub.status.idle": "2024-06-17T07:17:32.369088Z",
     "shell.execute_reply": "2024-06-17T07:17:32.367983Z",
     "shell.execute_reply.started": "2024-06-17T07:17:32.345915Z"
    }
   },
   "outputs": [],
   "source": [
    "# set access token -- copy it from HuggingFace\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:18:13.482085Z",
     "iopub.status.busy": "2024-06-17T07:18:13.481727Z",
     "iopub.status.idle": "2024-06-17T07:21:00.269864Z",
     "shell.execute_reply": "2024-06-17T07:21:00.269048Z",
     "shell.execute_reply.started": "2024-06-17T07:18:13.482057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load \"manually\" model and tokenizer - now for a model that requires an access token\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-1.1-7b-it\",\n",
    "                device_map = \"auto\",\n",
    "                torch_dtype = torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T07:21:00.272620Z",
     "iopub.status.busy": "2024-06-17T07:21:00.272321Z",
     "iopub.status.idle": "2024-06-17T07:21:13.967743Z",
     "shell.execute_reply": "2024-06-17T07:21:13.966761Z",
     "shell.execute_reply.started": "2024-06-17T07:21:00.272595Z"
    }
   },
   "outputs": [],
   "source": [
    "# your prompt\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": prompt },\n",
    "]\n",
    "\n",
    "# apply the correct template to your prompt\n",
    "text = tokenizer.apply_chat_template(\n",
    "    chat,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# tokenize the prompt\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generate output\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# extract only the response tokens\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# decode tokens back to text\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
